{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gunnalakshmi/Calculator/blob/main/Final_Project_ANNSIH_(Short_term_forecast_of_gaseous_air_pollutants_(ground_level_O3_and_NO2)_using_satellite_and_reanalysis_data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, MultiHeadAttention, Add, LayerNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "SyS_vfzYwZa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "site_files = [\"site_1_train_data.csv\", \"site_2_train_data.csv\", \"site_3_train_data.csv\"]\n",
        "\n",
        "dfs = {}\n",
        "train_dfs = {}\n",
        "test_dfs = {}\n",
        "scaler_xs = {}\n",
        "scaler_ys = {}\n",
        "X_train_seqs = {}\n",
        "y_train_seqs = {}\n",
        "X_test_seqs = {}\n",
        "y_test_seqs = {}\n",
        "results = {}\n",
        "time_steps = 72"
      ],
      "metadata": {
        "id": "CqZiKayAtk2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in site_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs[file] = df.copy()\n",
        "    print(file, \"initial shape:\", df.shape)"
      ],
      "metadata": {
        "id": "QeL78vX2Il2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1986bd-b3e2-443c-ce6f-4948c69cb261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site_1_train_data.csv initial shape: (25081, 16)\n",
            "site_2_train_data.csv initial shape: (25969, 16)\n",
            "site_3_train_data.csv initial shape: (21913, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = ['NO2_satellite', 'HCHO_satellite', 'ratio_satellite']\n",
        "for file, df in dfs.items():\n",
        "    for col in drop_cols:\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=col, inplace=True)\n",
        "    dfs[file] = df\n",
        "    print(file, \"after dropping satellite cols:\", df.shape)"
      ],
      "metadata": {
        "id": "RnLDKLiOIrc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961c697e-1343-4e10-92f0-764719d402db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site_1_train_data.csv after dropping satellite cols: (25081, 13)\n",
            "site_2_train_data.csv after dropping satellite cols: (25969, 13)\n",
            "site_3_train_data.csv after dropping satellite cols: (21913, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file, df in dfs.items():\n",
        "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
        "    df = df.sort_values('datetime').reset_index(drop=True)\n",
        "    dfs[file] = df\n",
        "    print(file, \"after datetime sort:\", df.shape)"
      ],
      "metadata": {
        "id": "MDZQDLb2Ivxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b2985c2-656a-4388-9431-b534da552db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site_1_train_data.csv after datetime sort: (25081, 14)\n",
            "site_2_train_data.csv after datetime sort: (25969, 14)\n",
            "site_3_train_data.csv after datetime sort: (21913, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file, df in dfs.items():\n",
        "    df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "    dfs[file] = df\n",
        "    print(file, \"after interpolation & dropna:\", df.shape)"
      ],
      "metadata": {
        "id": "2c_uOxzrIy4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06641d48-4188-4c1f-9c6f-ff9f1ef14167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site_1_train_data.csv after interpolation & dropna: (25081, 14)\n",
            "site_2_train_data.csv after interpolation & dropna: (25969, 14)\n",
            "site_3_train_data.csv after interpolation & dropna: (21913, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file, df in dfs.items():\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['O3_diff'] = df['O3_target'] - df['O3_target'].shift(1)\n",
        "    df['NO2_diff'] = df['NO2_target'] - df['NO2_target'].shift(1)\n",
        "    df.fillna(0, inplace=True)\n",
        "    dfs[file] = df"
      ],
      "metadata": {
        "id": "DEVYmISoI0Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file, df in dfs.items():\n",
        "    for pollutant in ['O3_target', 'NO2_target']:\n",
        "        for lag in range(1, 73):\n",
        "            df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
        "    df.dropna(inplace=True)\n",
        "    dfs[file] = df"
      ],
      "metadata": {
        "id": "oG_wZj8zI3dk",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb0a25f-28ea-409e-9034-9f021de89b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n",
            "/tmp/ipython-input-1133038121.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{pollutant}_lag_{lag}'] = df[pollutant].shift(lag)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_steps = 72\n",
        "\n",
        "for file, df in dfs.items():\n",
        "    input_features = [\n",
        "        'O3_forecast', 'NO2_forecast', 'T_forecast', 'q_forecast',\n",
        "        'u_forecast', 'v_forecast', 'w_forecast',\n",
        "        'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
        "        'O3_diff', 'NO2_diff'\n",
        "    ]\n",
        "    lag_features = [col for col in df.columns if '_lag_' in col]\n",
        "    input_features.extend(lag_features)\n",
        "    target_features = ['O3_target', 'NO2_target']\n",
        "\n",
        "    # Split train/test\n",
        "    train_df, test_df = train_test_split(df, test_size=0.25, shuffle=False)\n",
        "    train_dfs[file] = train_df\n",
        "    test_dfs[file] = test_df\n",
        "\n",
        "    # Input scaler (one per site)\n",
        "    scaler_x = StandardScaler()\n",
        "    X_train = scaler_x.fit_transform(train_df[input_features])\n",
        "    X_test = scaler_x.transform(test_df[input_features])\n",
        "    scaler_xs[file] = scaler_x\n",
        "\n",
        "    # Create sequences for X\n",
        "    Xs_train, Xs_test = [], []\n",
        "    for i in range(len(X_train) - time_steps):\n",
        "        Xs_train.append(X_train[i:(i + time_steps)])\n",
        "    for i in range(len(X_test) - time_steps):\n",
        "        Xs_test.append(X_test[i:(i + time_steps)])\n",
        "\n",
        "    X_train_seq = np.array(Xs_train)\n",
        "    X_test_seq = np.array(Xs_test)\n",
        "    X_train_seqs[file] = X_train_seq\n",
        "    X_test_seqs[file] = X_test_seq\n",
        "\n",
        "    # Target scalers per pollutant (O3, NO2)\n",
        "    for pollutant in target_features:\n",
        "        scaler_y = StandardScaler()\n",
        "        y_train_full = scaler_y.fit_transform(train_df[[pollutant]])\n",
        "        y_test_full = scaler_y.transform(test_df[[pollutant]])\n",
        "\n",
        "        ys_train, ys_test = [], []\n",
        "        for i in range(len(y_train_full) - time_steps):\n",
        "            ys_train.append(y_train_full[i + time_steps])\n",
        "        for i in range(len(y_test_full) - time_steps):\n",
        "            ys_test.append(y_test_full[i + time_steps])\n",
        "\n",
        "        y_train_seq = np.array(ys_train)\n",
        "        y_test_seq = np.array(ys_test)\n",
        "\n",
        "        # Save the target-specific scaler\n",
        "        scaler_ys[(file, pollutant)] = scaler_y\n",
        "        y_train_seqs[(file, pollutant)] = y_train_seq\n",
        "        y_test_seqs[(file, pollutant)] = y_test_seq\n",
        "\n",
        "    print(file,\n",
        "          \"Train seq shape:\", X_train_seq.shape,\n",
        "          \"Test seq shape:\", X_test_seq.shape)\n"
      ],
      "metadata": {
        "id": "rQKIY-rQI7Lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d59cdf-c63d-4cde-dae6-5f79942d7e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site_1_train_data.csv Train seq shape: (18684, 72, 157) Test seq shape: (6181, 72, 157)\n",
            "site_2_train_data.csv Train seq shape: (19350, 72, 157) Test seq shape: (6403, 72, 157)\n",
            "site_3_train_data.csv Train seq shape: (16308, 72, 157) Test seq shape: (5389, 72, 157)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "os.makedirs(\"saved_scalers\", exist_ok=True)\n",
        "\n",
        "for file, x_scaler in scaler_xs.items():\n",
        "    site = file.replace(\"_train_data.csv\", \"\")\n",
        "    x_path = f\"saved_scalers/{site}_X_scaler.pkl\"\n",
        "    joblib.dump(x_scaler, x_path)\n",
        "    print(\"Saved X-scaler:\", x_path)\n",
        "\n",
        "for key, y_scaler in scaler_ys.items():\n",
        "    if isinstance(key, tuple):\n",
        "        file, pollutant = key\n",
        "        site = file.replace(\"_train_data.csv\", \"\")\n",
        "        poll_short = pollutant.replace(\"_target\", \"\")\n",
        "        y_path = f\"saved_scalers/{site}_{poll_short}_Y_scaler.pkl\"\n",
        "        joblib.dump(y_scaler, y_path)\n",
        "        print(f\"Saved Y-scaler for {site} - {poll_short}: {y_path}\")\n"
      ],
      "metadata": {
        "id": "qUz9Tl1XReSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56540c7-ea8f-486c-b23b-1aef262cebd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved X-scaler: saved_scalers/site_1_X_scaler.pkl\n",
            "Saved X-scaler: saved_scalers/site_2_X_scaler.pkl\n",
            "Saved X-scaler: saved_scalers/site_3_X_scaler.pkl\n",
            "Saved Y-scaler for site_1 - O3: saved_scalers/site_1_O3_Y_scaler.pkl\n",
            "Saved Y-scaler for site_1 - NO2: saved_scalers/site_1_NO2_Y_scaler.pkl\n",
            "Saved Y-scaler for site_2 - O3: saved_scalers/site_2_O3_Y_scaler.pkl\n",
            "Saved Y-scaler for site_2 - NO2: saved_scalers/site_2_NO2_Y_scaler.pkl\n",
            "Saved Y-scaler for site_3 - O3: saved_scalers/site_3_O3_Y_scaler.pkl\n",
            "Saved Y-scaler for site_3 - NO2: saved_scalers/site_3_NO2_Y_scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"saved_scalers_zip\", 'zip', \"saved_scalers\")\n",
        "print(\"Zipped  saved_scalers_zip.zip\")\n"
      ],
      "metadata": {
        "id": "J3Cuer74TWnp",
        "outputId": "43ab5cf6-9467-4a65-86ec-35d9ac0a1e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipped  saved_scalers_zip.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, file in enumerate(site_files, start=1):\n",
        "    X_train_seq = X_train_seqs[file]\n",
        "    X_test_seq = X_test_seqs[file]\n",
        "    train_df = train_dfs[file]\n",
        "    test_df = test_dfs[file]\n",
        "\n",
        "    for pollutant in ['O3_target', 'NO2_target']:\n",
        "        print(f\"\\n\\n==================== {file} - {pollutant} ====================\")\n",
        "\n",
        "        scaler_y = StandardScaler()\n",
        "        scaler_ys[(file, pollutant)] = scaler_y\n",
        "        y_train_full = scaler_y.fit_transform(train_df[[pollutant]])\n",
        "        y_test_full = scaler_y.transform(test_df[[pollutant]])\n",
        "\n",
        "        ys_train = []\n",
        "        for i in range(len(y_train_full) - time_steps):\n",
        "            ys_train.append(y_train_full[i + time_steps])\n",
        "        y_train_seq = np.array(ys_train)\n",
        "\n",
        "        ys_test = []\n",
        "        for i in range(len(y_test_full) - time_steps):\n",
        "            ys_test.append(y_test_full[i + time_steps])\n",
        "        y_test_seq = np.array(ys_test)\n",
        "\n",
        "        y_train_seqs[(file, pollutant)] = y_train_seq\n",
        "        y_test_seqs[(file, pollutant)] = y_test_seq\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
        "        ]\n",
        "\n",
        "        inputs = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
        "        x = LSTM(128, return_sequences=True)(inputs)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = LSTM(64)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        outputs = Dense(1)(x)\n",
        "        lstm_model = Model(inputs, outputs)\n",
        "        lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        print(\"Training LSTM...\")\n",
        "        lstm_model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq), epochs=100, batch_size=64, callbacks=callbacks, verbose=2)\n",
        "        y_pred_lstm = lstm_model.predict(X_test_seq)\n",
        "\n",
        "        inputs = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
        "        x = GRU(128, return_sequences=True)(inputs)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = GRU(64)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        outputs = Dense(1)(x)\n",
        "        gru_model = Model(inputs, outputs)\n",
        "        gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        print(\"Training GRU...\")\n",
        "        gru_model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq), epochs=100, batch_size=64, callbacks=callbacks, verbose=2)\n",
        "        y_pred_gru = gru_model.predict(X_test_seq)\n",
        "\n",
        "        inputs = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
        "        attn = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)\n",
        "        attn = Dropout(0.2)(attn)\n",
        "        x = Add()([inputs, attn])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn = Dense(128, activation='relu')(x)\n",
        "        ffn = Dropout(0.2)(ffn)\n",
        "        ffn = Dense(X_train_seq.shape[2])(ffn)\n",
        "        x = Add()([x, ffn])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        outputs = Dense(1)(x)\n",
        "        trans_model = Model(inputs, outputs)\n",
        "        trans_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        print(\"Training Transformer...\")\n",
        "        trans_model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq), epochs=100, batch_size=64, callbacks=callbacks, verbose=2)\n",
        "        y_pred_trans = trans_model.predict(X_test_seq)\n",
        "\n",
        "        y_pred_ensemble_scaled = 0.4 * y_pred_gru + 0.4 * y_pred_lstm + 0.2 * y_pred_trans\n",
        "\n",
        "        y_test_inv = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).reshape(-1)\n",
        "        y_pred_inv = scaler_y.inverse_transform(y_pred_ensemble_scaled.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "        results[(file, pollutant, 'Ensemble')] = {\"y_true\": y_test_inv, \"y_pred\": y_pred_inv}\n",
        "\n",
        "        import os\n",
        "        site_name = file.replace(\"_train_data.csv\",\"\")   # e.g. \"site_1\"\n",
        "        poll_short = pollutant.replace(\"_target\",\"\")     # e.g. \"O3\" or \"NO2\"\n",
        "        save_dir = os.path.join(\"saved_models\", site_name)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        lstm_path = os.path.join(save_dir, f\"{site_name}_{poll_short}_LSTM.keras\")\n",
        "        gru_path  = os.path.join(save_dir, f\"{site_name}_{poll_short}_GRU.keras\")\n",
        "        trans_path= os.path.join(save_dir, f\"{site_name}_{poll_short}_TRANS.keras\")\n",
        "\n",
        "        lstm_model.save(lstm_path)\n",
        "        gru_model.save(gru_path)\n",
        "        trans_model.save(trans_path)\n",
        "\n",
        "        print(f\"\\n Saved: {lstm_path}\")\n",
        "        print(f\" Saved: {gru_path}\")\n",
        "        print(f\" Saved: {trans_path}\")\n",
        "        # --- end save ---"
      ],
      "metadata": {
        "id": "xuL5pBhEI-FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d5c358-ad3b-48b9-87b1-c5f350e0e98b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==================== site_1_train_data.csv - O3_target ====================\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "292/292 - 10s - 35ms/step - loss: 0.2924 - val_loss: 0.0994 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1712 - val_loss: 0.0666 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.1438 - val_loss: 0.0514 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1303 - val_loss: 0.0502 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1206 - val_loss: 0.0448 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1200 - val_loss: 0.0444 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1151 - val_loss: 0.0433 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1112 - val_loss: 0.0404 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "292/292 - 4s - 13ms/step - loss: 0.1064 - val_loss: 0.0523 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1043 - val_loss: 0.0396 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1031 - val_loss: 0.0477 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0982 - val_loss: 0.0505 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.0919 - val_loss: 0.0423 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0953 - val_loss: 0.0422 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0904 - val_loss: 0.0432 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.0827 - val_loss: 0.0465 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0752 - val_loss: 0.0460 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0747 - val_loss: 0.0473 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "292/292 - 6s - 20ms/step - loss: 0.0747 - val_loss: 0.0457 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0723 - val_loss: 0.0515 - learning_rate: 5.0000e-04\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training GRU...\n",
            "Epoch 1/100\n",
            "292/292 - 7s - 26ms/step - loss: 0.2852 - val_loss: 0.0790 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1662 - val_loss: 0.0485 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1456 - val_loss: 0.0377 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1361 - val_loss: 0.0400 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1342 - val_loss: 0.0511 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1288 - val_loss: 0.0393 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1216 - val_loss: 0.0377 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1192 - val_loss: 0.0416 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1100 - val_loss: 0.0396 - learning_rate: 5.0000e-04\n",
            "Epoch 10/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.1084 - val_loss: 0.0375 - learning_rate: 5.0000e-04\n",
            "Epoch 11/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1071 - val_loss: 0.0370 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1040 - val_loss: 0.0405 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1016 - val_loss: 0.0329 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1020 - val_loss: 0.0387 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0995 - val_loss: 0.0356 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.0987 - val_loss: 0.0350 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0981 - val_loss: 0.0379 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "292/292 - 5s - 18ms/step - loss: 0.0930 - val_loss: 0.0356 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.0917 - val_loss: 0.0413 - learning_rate: 2.5000e-04\n",
            "Epoch 20/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0864 - val_loss: 0.0390 - learning_rate: 2.5000e-04\n",
            "Epoch 21/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0857 - val_loss: 0.0372 - learning_rate: 2.5000e-04\n",
            "Epoch 22/100\n",
            "292/292 - 6s - 20ms/step - loss: 0.0857 - val_loss: 0.0357 - learning_rate: 2.5000e-04\n",
            "Epoch 23/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.0848 - val_loss: 0.0386 - learning_rate: 2.5000e-04\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Training Transformer...\n",
            "Epoch 1/100\n",
            "292/292 - 19s - 64ms/step - loss: 0.8479 - val_loss: 0.3865 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.8127 - val_loss: 0.2729 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.7267 - val_loss: 0.2269 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.5415 - val_loss: 0.1513 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.4249 - val_loss: 0.1526 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.3824 - val_loss: 0.1735 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.3269 - val_loss: 0.1410 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.3052 - val_loss: 0.2207 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.2899 - val_loss: 0.1522 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2544 - val_loss: 0.1540 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2493 - val_loss: 0.1506 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2297 - val_loss: 0.1418 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1963 - val_loss: 0.1445 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.1922 - val_loss: 0.1463 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1905 - val_loss: 0.1887 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1838 - val_loss: 0.1584 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1812 - val_loss: 0.1652 - learning_rate: 5.0000e-04\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\n",
            " Saved: saved_models/site_1/site_1_O3_LSTM.keras\n",
            " Saved: saved_models/site_1/site_1_O3_GRU.keras\n",
            " Saved: saved_models/site_1/site_1_O3_TRANS.keras\n",
            "\n",
            "\n",
            "==================== site_1_train_data.csv - NO2_target ====================\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "292/292 - 7s - 23ms/step - loss: 0.3142 - val_loss: 0.1670 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1630 - val_loss: 0.1228 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.1449 - val_loss: 0.1200 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1317 - val_loss: 0.1176 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1225 - val_loss: 0.1070 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1149 - val_loss: 0.1093 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1103 - val_loss: 0.1105 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1060 - val_loss: 0.1033 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.1040 - val_loss: 0.1146 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1034 - val_loss: 0.1146 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1028 - val_loss: 0.1148 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1024 - val_loss: 0.1139 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0944 - val_loss: 0.1046 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "292/292 - 5s - 17ms/step - loss: 0.0846 - val_loss: 0.1168 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.0807 - val_loss: 0.1112 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "292/292 - 4s - 13ms/step - loss: 0.0798 - val_loss: 0.1057 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0794 - val_loss: 0.1128 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.0768 - val_loss: 0.1159 - learning_rate: 5.0000e-04\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training GRU...\n",
            "Epoch 1/100\n",
            "292/292 - 6s - 22ms/step - loss: 0.2788 - val_loss: 0.1293 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1653 - val_loss: 0.1304 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1454 - val_loss: 0.1051 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1360 - val_loss: 0.1141 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1284 - val_loss: 0.1117 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1212 - val_loss: 0.1188 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1198 - val_loss: 0.1102 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1141 - val_loss: 0.1093 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.1019 - val_loss: 0.1089 - learning_rate: 5.0000e-04\n",
            "Epoch 10/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.1006 - val_loss: 0.1016 - learning_rate: 5.0000e-04\n",
            "Epoch 11/100\n",
            "292/292 - 6s - 20ms/step - loss: 0.0994 - val_loss: 0.1107 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0998 - val_loss: 0.1067 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0998 - val_loss: 0.0993 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.0988 - val_loss: 0.1139 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0973 - val_loss: 0.1112 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0976 - val_loss: 0.1099 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.0931 - val_loss: 0.1040 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0934 - val_loss: 0.1097 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "292/292 - 3s - 12ms/step - loss: 0.0827 - val_loss: 0.1021 - learning_rate: 2.5000e-04\n",
            "Epoch 20/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.0836 - val_loss: 0.1033 - learning_rate: 2.5000e-04\n",
            "Epoch 21/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.0843 - val_loss: 0.1030 - learning_rate: 2.5000e-04\n",
            "Epoch 22/100\n",
            "292/292 - 4s - 12ms/step - loss: 0.0808 - val_loss: 0.1065 - learning_rate: 2.5000e-04\n",
            "Epoch 23/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.0840 - val_loss: 0.1048 - learning_rate: 2.5000e-04\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training Transformer...\n",
            "Epoch 1/100\n",
            "292/292 - 16s - 54ms/step - loss: 0.5806 - val_loss: 0.5235 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.5090 - val_loss: 0.5068 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "292/292 - 3s - 10ms/step - loss: 0.4739 - val_loss: 0.4802 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.4228 - val_loss: 0.4745 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.3780 - val_loss: 0.3943 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.3287 - val_loss: 0.3883 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.3004 - val_loss: 0.3921 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.2794 - val_loss: 0.3742 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2580 - val_loss: 0.3761 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2534 - val_loss: 0.3362 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2315 - val_loss: 0.3432 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2360 - val_loss: 0.3577 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.2305 - val_loss: 0.3454 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2243 - val_loss: 0.3362 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.2129 - val_loss: 0.3618 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1838 - val_loss: 0.3218 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1675 - val_loss: 0.3275 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.1715 - val_loss: 0.3313 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1731 - val_loss: 0.3259 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1665 - val_loss: 0.3285 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1609 - val_loss: 0.3208 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.1597 - val_loss: 0.3327 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.1610 - val_loss: 0.3270 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "292/292 - 5s - 17ms/step - loss: 0.1616 - val_loss: 0.3426 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1555 - val_loss: 0.3302 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.1497 - val_loss: 0.3250 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "292/292 - 3s - 9ms/step - loss: 0.1363 - val_loss: 0.3293 - learning_rate: 2.5000e-04\n",
            "Epoch 28/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1354 - val_loss: 0.3351 - learning_rate: 2.5000e-04\n",
            "Epoch 29/100\n",
            "292/292 - 2s - 8ms/step - loss: 0.1331 - val_loss: 0.3346 - learning_rate: 2.5000e-04\n",
            "Epoch 30/100\n",
            "292/292 - 4s - 15ms/step - loss: 0.1323 - val_loss: 0.3384 - learning_rate: 2.5000e-04\n",
            "Epoch 31/100\n",
            "292/292 - 4s - 14ms/step - loss: 0.1287 - val_loss: 0.3265 - learning_rate: 2.5000e-04\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\n",
            " Saved: saved_models/site_1/site_1_NO2_LSTM.keras\n",
            " Saved: saved_models/site_1/site_1_NO2_GRU.keras\n",
            " Saved: saved_models/site_1/site_1_NO2_TRANS.keras\n",
            "\n",
            "\n",
            "==================== site_2_train_data.csv - O3_target ====================\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "303/303 - 8s - 25ms/step - loss: 0.2589 - val_loss: 0.1177 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1372 - val_loss: 0.0732 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1093 - val_loss: 0.0688 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "303/303 - 5s - 17ms/step - loss: 0.1008 - val_loss: 0.0669 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0929 - val_loss: 0.0643 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0922 - val_loss: 0.0631 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0885 - val_loss: 0.0605 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0847 - val_loss: 0.0610 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.0857 - val_loss: 0.0636 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0817 - val_loss: 0.0642 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0825 - val_loss: 0.0617 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "303/303 - 5s - 15ms/step - loss: 0.0805 - val_loss: 0.0668 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0687 - val_loss: 0.0647 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0716 - val_loss: 0.0679 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "303/303 - 5s - 16ms/step - loss: 0.0699 - val_loss: 0.0698 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0671 - val_loss: 0.0640 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0670 - val_loss: 0.0627 - learning_rate: 5.0000e-04\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training GRU...\n",
            "Epoch 1/100\n",
            "303/303 - 7s - 24ms/step - loss: 0.2443 - val_loss: 0.0947 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "303/303 - 5s - 15ms/step - loss: 0.1369 - val_loss: 0.0784 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1169 - val_loss: 0.0693 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.1081 - val_loss: 0.0619 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.1023 - val_loss: 0.0621 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "303/303 - 5s - 15ms/step - loss: 0.0957 - val_loss: 0.0603 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0965 - val_loss: 0.0641 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.0970 - val_loss: 0.0687 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0887 - val_loss: 0.0595 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0894 - val_loss: 0.0630 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.0876 - val_loss: 0.0682 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0890 - val_loss: 0.0601 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0823 - val_loss: 0.0585 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0834 - val_loss: 0.0638 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0836 - val_loss: 0.0597 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0799 - val_loss: 0.0601 - learning_rate: 1.0000e-03\n",
            "Epoch 17/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0775 - val_loss: 0.0649 - learning_rate: 1.0000e-03\n",
            "Epoch 18/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0781 - val_loss: 0.0635 - learning_rate: 1.0000e-03\n",
            "Epoch 19/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0691 - val_loss: 0.0593 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0703 - val_loss: 0.0628 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0668 - val_loss: 0.0604 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0660 - val_loss: 0.0591 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.0650 - val_loss: 0.0597 - learning_rate: 5.0000e-04\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training Transformer...\n",
            "Epoch 1/100\n",
            "303/303 - 17s - 56ms/step - loss: 0.8308 - val_loss: 0.5701 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.7006 - val_loss: 0.4493 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.4808 - val_loss: 0.3799 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.4032 - val_loss: 0.4436 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.3580 - val_loss: 0.3627 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.3220 - val_loss: 0.4274 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2935 - val_loss: 0.3520 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "303/303 - 3s - 8ms/step - loss: 0.2692 - val_loss: 0.3474 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2518 - val_loss: 0.3758 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2499 - val_loss: 0.3999 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2273 - val_loss: 0.3800 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2202 - val_loss: 0.3335 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2105 - val_loss: 0.3288 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2014 - val_loss: 0.3326 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2022 - val_loss: 0.2946 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1849 - val_loss: 0.3027 - learning_rate: 1.0000e-03\n",
            "Epoch 17/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1895 - val_loss: 0.3251 - learning_rate: 1.0000e-03\n",
            "Epoch 18/100\n",
            "303/303 - 3s - 8ms/step - loss: 0.1771 - val_loss: 0.2845 - learning_rate: 1.0000e-03\n",
            "Epoch 19/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1682 - val_loss: 0.3092 - learning_rate: 1.0000e-03\n",
            "Epoch 20/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1680 - val_loss: 0.3202 - learning_rate: 1.0000e-03\n",
            "Epoch 21/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1647 - val_loss: 0.3039 - learning_rate: 1.0000e-03\n",
            "Epoch 22/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1593 - val_loss: 0.3246 - learning_rate: 1.0000e-03\n",
            "Epoch 23/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1544 - val_loss: 0.3183 - learning_rate: 1.0000e-03\n",
            "Epoch 24/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1312 - val_loss: 0.3281 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1271 - val_loss: 0.3621 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1214 - val_loss: 0.3243 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1206 - val_loss: 0.3157 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "303/303 - 3s - 8ms/step - loss: 0.1178 - val_loss: 0.3448 - learning_rate: 5.0000e-04\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\n",
            " Saved: saved_models/site_2/site_2_O3_LSTM.keras\n",
            " Saved: saved_models/site_2/site_2_O3_GRU.keras\n",
            " Saved: saved_models/site_2/site_2_O3_TRANS.keras\n",
            "\n",
            "\n",
            "==================== site_2_train_data.csv - NO2_target ====================\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "303/303 - 7s - 23ms/step - loss: 0.2907 - val_loss: 0.1730 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1702 - val_loss: 0.1307 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.1342 - val_loss: 0.1157 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "303/303 - 5s - 15ms/step - loss: 0.1293 - val_loss: 0.1105 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "303/303 - 5s - 15ms/step - loss: 0.1216 - val_loss: 0.1128 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.1173 - val_loss: 0.1074 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.1162 - val_loss: 0.1043 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "303/303 - 5s - 16ms/step - loss: 0.1085 - val_loss: 0.1049 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.1075 - val_loss: 0.1083 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1067 - val_loss: 0.1036 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.1038 - val_loss: 0.1057 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0993 - val_loss: 0.1077 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1004 - val_loss: 0.1050 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.1000 - val_loss: 0.1084 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0987 - val_loss: 0.1096 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0861 - val_loss: 0.1074 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.0799 - val_loss: 0.1048 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0798 - val_loss: 0.1117 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0783 - val_loss: 0.1129 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.0809 - val_loss: 0.1095 - learning_rate: 5.0000e-04\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training GRU...\n",
            "Epoch 1/100\n",
            "303/303 - 7s - 22ms/step - loss: 0.2750 - val_loss: 0.1479 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.1715 - val_loss: 0.1140 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.1535 - val_loss: 0.1061 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.1396 - val_loss: 0.1033 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.1360 - val_loss: 0.1028 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1293 - val_loss: 0.1032 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1220 - val_loss: 0.1011 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.1247 - val_loss: 0.1058 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1230 - val_loss: 0.1033 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "303/303 - 5s - 17ms/step - loss: 0.1160 - val_loss: 0.0945 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "303/303 - 4s - 13ms/step - loss: 0.1107 - val_loss: 0.0997 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1140 - val_loss: 0.0961 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1108 - val_loss: 0.1003 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "303/303 - 5s - 15ms/step - loss: 0.1100 - val_loss: 0.1009 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.1057 - val_loss: 0.1166 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0968 - val_loss: 0.0955 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "303/303 - 4s - 15ms/step - loss: 0.0958 - val_loss: 0.0969 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0934 - val_loss: 0.0976 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "303/303 - 4s - 12ms/step - loss: 0.0913 - val_loss: 0.1038 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "303/303 - 4s - 14ms/step - loss: 0.0917 - val_loss: 0.0998 - learning_rate: 5.0000e-04\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Training Transformer...\n",
            "Epoch 1/100\n",
            "303/303 - 16s - 54ms/step - loss: 0.5609 - val_loss: 0.4819 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.4713 - val_loss: 0.3989 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.3909 - val_loss: 0.3711 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.3506 - val_loss: 0.3734 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.3073 - val_loss: 0.3232 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2888 - val_loss: 0.3500 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2737 - val_loss: 0.3290 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2579 - val_loss: 0.3329 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2449 - val_loss: 0.3192 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "303/303 - 3s - 8ms/step - loss: 0.2358 - val_loss: 0.3063 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2314 - val_loss: 0.3107 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2211 - val_loss: 0.3158 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2181 - val_loss: 0.3244 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2164 - val_loss: 0.2958 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.2127 - val_loss: 0.3104 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2027 - val_loss: 0.3190 - learning_rate: 1.0000e-03\n",
            "Epoch 17/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.2020 - val_loss: 0.3093 - learning_rate: 1.0000e-03\n",
            "Epoch 18/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1978 - val_loss: 0.2994 - learning_rate: 1.0000e-03\n",
            "Epoch 19/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1810 - val_loss: 0.2898 - learning_rate: 1.0000e-03\n",
            "Epoch 20/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1937 - val_loss: 0.2984 - learning_rate: 1.0000e-03\n",
            "Epoch 21/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1894 - val_loss: 0.3028 - learning_rate: 1.0000e-03\n",
            "Epoch 22/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1805 - val_loss: 0.3057 - learning_rate: 1.0000e-03\n",
            "Epoch 23/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1775 - val_loss: 0.3072 - learning_rate: 1.0000e-03\n",
            "Epoch 24/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1769 - val_loss: 0.3007 - learning_rate: 1.0000e-03\n",
            "Epoch 25/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1572 - val_loss: 0.3032 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "303/303 - 3s - 9ms/step - loss: 0.1496 - val_loss: 0.2958 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1488 - val_loss: 0.3287 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1465 - val_loss: 0.2989 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "303/303 - 2s - 8ms/step - loss: 0.1443 - val_loss: 0.3126 - learning_rate: 5.0000e-04\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\n",
            " Saved: saved_models/site_2/site_2_NO2_LSTM.keras\n",
            " Saved: saved_models/site_2/site_2_NO2_GRU.keras\n",
            " Saved: saved_models/site_2/site_2_NO2_TRANS.keras\n",
            "\n",
            "\n",
            "==================== site_3_train_data.csv - O3_target ====================\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "255/255 - 7s - 26ms/step - loss: 0.2979 - val_loss: 0.1516 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.1489 - val_loss: 0.0978 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.1150 - val_loss: 0.0817 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.1076 - val_loss: 0.0869 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0975 - val_loss: 0.0778 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0941 - val_loss: 0.0778 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "255/255 - 4s - 17ms/step - loss: 0.0936 - val_loss: 0.0785 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0891 - val_loss: 0.0758 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "255/255 - 4s - 16ms/step - loss: 0.0849 - val_loss: 0.0795 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0855 - val_loss: 0.0843 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0807 - val_loss: 0.0725 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0788 - val_loss: 0.0773 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0792 - val_loss: 0.0855 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0754 - val_loss: 0.0781 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "255/255 - 6s - 24ms/step - loss: 0.0748 - val_loss: 0.0802 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0741 - val_loss: 0.0754 - learning_rate: 1.0000e-03\n",
            "Epoch 17/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0656 - val_loss: 0.0772 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0641 - val_loss: 0.0776 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "255/255 - 4s - 16ms/step - loss: 0.0618 - val_loss: 0.0785 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0623 - val_loss: 0.0768 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "255/255 - 5s - 20ms/step - loss: 0.0606 - val_loss: 0.0808 - learning_rate: 5.0000e-04\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "Training GRU...\n",
            "Epoch 1/100\n",
            "255/255 - 6s - 23ms/step - loss: 0.2740 - val_loss: 0.1161 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.1439 - val_loss: 0.0841 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "255/255 - 4s - 17ms/step - loss: 0.1280 - val_loss: 0.0798 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1147 - val_loss: 0.0717 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "255/255 - 3s - 14ms/step - loss: 0.1093 - val_loss: 0.0743 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.1052 - val_loss: 0.0762 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1001 - val_loss: 0.0715 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0983 - val_loss: 0.0694 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0958 - val_loss: 0.0750 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0928 - val_loss: 0.0696 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0935 - val_loss: 0.0743 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0901 - val_loss: 0.0771 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0891 - val_loss: 0.0745 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0810 - val_loss: 0.0771 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0798 - val_loss: 0.0726 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.0785 - val_loss: 0.0704 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0776 - val_loss: 0.0705 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0765 - val_loss: 0.0697 - learning_rate: 5.0000e-04\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training Transformer...\n",
            "Epoch 1/100\n",
            "255/255 - 17s - 68ms/step - loss: 0.9326 - val_loss: 0.6318 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "255/255 - 9s - 36ms/step - loss: 0.8573 - val_loss: 0.5337 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.6422 - val_loss: 0.4309 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.4980 - val_loss: 0.5243 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.3845 - val_loss: 0.4312 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.3410 - val_loss: 0.3659 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.3139 - val_loss: 0.3378 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "255/255 - 3s - 10ms/step - loss: 0.2935 - val_loss: 0.4277 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2591 - val_loss: 0.3841 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2507 - val_loss: 0.3556 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2348 - val_loss: 0.4059 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2228 - val_loss: 0.3581 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.1864 - val_loss: 0.3452 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.1779 - val_loss: 0.3623 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.1788 - val_loss: 0.3553 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.1664 - val_loss: 0.3607 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.1682 - val_loss: 0.3484 - learning_rate: 5.0000e-04\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "\n",
            " Saved: saved_models/site_3/site_3_O3_LSTM.keras\n",
            " Saved: saved_models/site_3/site_3_O3_GRU.keras\n",
            " Saved: saved_models/site_3/site_3_O3_TRANS.keras\n",
            "\n",
            "\n",
            "==================== site_3_train_data.csv - NO2_target ====================\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "255/255 - 6s - 24ms/step - loss: 0.3042 - val_loss: 0.1305 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.1686 - val_loss: 0.1023 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "255/255 - 5s - 19ms/step - loss: 0.1400 - val_loss: 0.0908 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1284 - val_loss: 0.0723 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.1207 - val_loss: 0.0760 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.1182 - val_loss: 0.0889 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1117 - val_loss: 0.0737 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1077 - val_loss: 0.0747 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.1069 - val_loss: 0.0703 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "255/255 - 4s - 17ms/step - loss: 0.1028 - val_loss: 0.0744 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1042 - val_loss: 0.0722 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.1022 - val_loss: 0.0725 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.0958 - val_loss: 0.0682 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0954 - val_loss: 0.0703 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0909 - val_loss: 0.0696 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0905 - val_loss: 0.0710 - learning_rate: 1.0000e-03\n",
            "Epoch 17/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0877 - val_loss: 0.0693 - learning_rate: 1.0000e-03\n",
            "Epoch 18/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0873 - val_loss: 0.0735 - learning_rate: 1.0000e-03\n",
            "Epoch 19/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0754 - val_loss: 0.0707 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0739 - val_loss: 0.0721 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0721 - val_loss: 0.0728 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0706 - val_loss: 0.0722 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0692 - val_loss: 0.0743 - learning_rate: 5.0000e-04\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Training GRU...\n",
            "Epoch 1/100\n",
            "255/255 - 6s - 23ms/step - loss: 0.2781 - val_loss: 0.1020 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "255/255 - 5s - 19ms/step - loss: 0.1644 - val_loss: 0.0796 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1448 - val_loss: 0.0673 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1327 - val_loss: 0.0709 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.1260 - val_loss: 0.0652 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.1212 - val_loss: 0.0665 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1209 - val_loss: 0.0671 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1187 - val_loss: 0.0691 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.1155 - val_loss: 0.0687 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1115 - val_loss: 0.0701 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.1050 - val_loss: 0.0606 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.1021 - val_loss: 0.0625 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.1018 - val_loss: 0.0612 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0970 - val_loss: 0.0602 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0967 - val_loss: 0.0624 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0943 - val_loss: 0.0628 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.0931 - val_loss: 0.0596 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0951 - val_loss: 0.0608 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0902 - val_loss: 0.0613 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "255/255 - 4s - 14ms/step - loss: 0.0895 - val_loss: 0.0677 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "255/255 - 3s - 13ms/step - loss: 0.0882 - val_loss: 0.0670 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0877 - val_loss: 0.0632 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0808 - val_loss: 0.0609 - learning_rate: 2.5000e-04\n",
            "Epoch 24/100\n",
            "255/255 - 4s - 15ms/step - loss: 0.0808 - val_loss: 0.0633 - learning_rate: 2.5000e-04\n",
            "Epoch 25/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0797 - val_loss: 0.0623 - learning_rate: 2.5000e-04\n",
            "Epoch 26/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0778 - val_loss: 0.0620 - learning_rate: 2.5000e-04\n",
            "Epoch 27/100\n",
            "255/255 - 3s - 12ms/step - loss: 0.0762 - val_loss: 0.0631 - learning_rate: 2.5000e-04\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "Training Transformer...\n",
            "Epoch 1/100\n",
            "255/255 - 16s - 62ms/step - loss: 0.6530 - val_loss: 0.3625 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.5238 - val_loss: 0.3121 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.4322 - val_loss: 0.3036 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "255/255 - 2s - 10ms/step - loss: 0.3689 - val_loss: 0.2717 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.3281 - val_loss: 0.2673 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.3055 - val_loss: 0.2575 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2936 - val_loss: 0.2544 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2615 - val_loss: 0.3002 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.2578 - val_loss: 0.2945 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.2454 - val_loss: 0.2410 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "255/255 - 3s - 10ms/step - loss: 0.2342 - val_loss: 0.2342 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2250 - val_loss: 0.2551 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2138 - val_loss: 0.2388 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.2099 - val_loss: 0.2535 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "255/255 - 2s - 10ms/step - loss: 0.2095 - val_loss: 0.2929 - learning_rate: 1.0000e-03\n",
            "Epoch 16/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.1979 - val_loss: 0.2565 - learning_rate: 1.0000e-03\n",
            "Epoch 17/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.1642 - val_loss: 0.2496 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "255/255 - 2s - 8ms/step - loss: 0.1636 - val_loss: 0.2592 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.1591 - val_loss: 0.2654 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "255/255 - 3s - 10ms/step - loss: 0.1559 - val_loss: 0.2541 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "255/255 - 2s - 9ms/step - loss: 0.1562 - val_loss: 0.2555 - learning_rate: 5.0000e-04\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\n",
            " Saved: saved_models/site_3/site_3_NO2_LSTM.keras\n",
            " Saved: saved_models/site_3/site_3_NO2_GRU.keras\n",
            " Saved: saved_models/site_3/site_3_NO2_TRANS.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_rows = []\n",
        "\n",
        "for (file, pollutant, model_name), data in results.items():\n",
        "    y_true_inv = data[\"y_true\"]\n",
        "    y_pred_inv = data[\"y_pred\"]\n",
        "    site = file.split(\"_train_data.csv\")[0]\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
        "    mae = mean_absolute_error(y_true_inv, y_pred_inv)\n",
        "    r2 = r2_score(y_true_inv, y_pred_inv)\n",
        "    obs_mean = y_true_inv.mean()\n",
        "    numerator = np.sum((y_pred_inv - y_true_inv) ** 2)\n",
        "    denominator = np.sum((np.abs(y_pred_inv - y_true_inv) + np.abs(y_true_inv - obs_mean)) ** 2)\n",
        "    ria = 1 - (numerator / denominator) if denominator != 0 else np.nan\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"Site\": site,\n",
        "        \"Model\": model_name,\n",
        "        \"Target\": pollutant,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2,\n",
        "        \"RIA\": ria\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "print(\"\\n PERFORMANCE SUMMARY (All Sites & Models) \")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "avg_summary = summary_df.groupby([\"Site\", \"Model\"])[[\"RMSE\",\"MAE\",\"R2\",\"RIA\"]].mean().reset_index()\n",
        "print(\"\\nAVERAGE PERFORMANCE PER SITE\")\n",
        "print(avg_summary.to_string(index=False))\n",
        "\n",
        "best_per_site = avg_summary.loc[avg_summary.groupby(\"Site\")[\"R2\"].idxmax()]\n",
        "print(\"\\n BEST MODEL PER SITE (By R) \")\n",
        "print(best_per_site.to_string(index=False))\n",
        "\n",
        "overall_best = avg_summary.groupby(\"Model\")[[\"R2\",\"RIA\"]].mean().reset_index().sort_values(by=\"R2\", ascending=False).head(1)\n",
        "print(\"\\n OVERALL BEST MODEL\")\n",
        "print(overall_best.to_string(index=False))"
      ],
      "metadata": {
        "id": "gkjD6VahJBV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d5d1c2-49de-480b-882c-207b93a6308e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " PERFORMANCE SUMMARY (All Sites & Models) \n",
            "  Site    Model     Target      RMSE      MAE       R2      RIA\n",
            "site_1 Ensemble  O3_target  6.436671 3.474949 0.935279 0.954354\n",
            "site_1 Ensemble NO2_target  9.712048 5.852114 0.857821 0.916936\n",
            "site_2 Ensemble  O3_target  8.110899 5.317921 0.906114 0.937858\n",
            "site_2 Ensemble NO2_target  8.161874 5.637029 0.851812 0.914250\n",
            "site_3 Ensemble  O3_target 11.565543 7.867597 0.901306 0.934143\n",
            "site_3 Ensemble NO2_target  9.632655 6.706702 0.880852 0.925939\n",
            "\n",
            "AVERAGE PERFORMANCE PER SITE\n",
            "  Site    Model      RMSE      MAE       R2      RIA\n",
            "site_1 Ensemble  8.074359 4.663531 0.896550 0.935645\n",
            "site_2 Ensemble  8.136387 5.477475 0.878963 0.926054\n",
            "site_3 Ensemble 10.599099 7.287150 0.891079 0.930041\n",
            "\n",
            " BEST MODEL PER SITE (By R) \n",
            "  Site    Model      RMSE      MAE       R2      RIA\n",
            "site_1 Ensemble  8.074359 4.663531 0.896550 0.935645\n",
            "site_2 Ensemble  8.136387 5.477475 0.878963 0.926054\n",
            "site_3 Ensemble 10.599099 7.287150 0.891079 0.930041\n",
            "\n",
            " OVERALL BEST MODEL\n",
            "   Model       R2     RIA\n",
            "Ensemble 0.888864 0.93058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"saved_models\", 'zip', \"saved_models\")"
      ],
      "metadata": {
        "id": "0FZGsvMNgDRo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ef0f0bb8-f767-467b-c4e3-c6ce7047ee46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/saved_models.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"saved_models.zip\")\n"
      ],
      "metadata": {
        "id": "iLk1sH-dgHra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cc6257e0-1f40-4b9e-ecfc-f0af91b499e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9ae5d3f-4310-447f-89a9-5f72e23d8d2b\", \"saved_models.zip\", 37173391)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "all_results_list = []\n",
        "\n",
        "for (file, pollutant, model_type), data in results.items():\n",
        "    y_true = data[\"y_true\"]\n",
        "    y_pred = data[\"y_pred\"]\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2  = r2_score(y_true, y_pred)\n",
        "\n",
        "    site_name = file.replace(\"_train_data.csv\", \"\")\n",
        "    pollutant_short = pollutant.replace(\"_target\", \"\")\n",
        "\n",
        "    all_results_list.append({\n",
        "        \"Site\": site_name,\n",
        "        \"Pollutant\": pollutant_short,\n",
        "        \"Model\": model_type,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(all_results_list)\n",
        "summary_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "8PFadmcbVayl",
        "outputId": "89e60ce9-7391-4eeb-b11a-1de148258a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Site Pollutant     Model       RMSE       MAE        R2\n",
              "0  site_1        O3  Ensemble   6.436671  3.474949  0.935279\n",
              "1  site_1       NO2  Ensemble   9.712048  5.852114  0.857821\n",
              "2  site_2        O3  Ensemble   8.110899  5.317921  0.906114\n",
              "3  site_2       NO2  Ensemble   8.161874  5.637029  0.851812\n",
              "4  site_3        O3  Ensemble  11.565543  7.867597  0.901306\n",
              "5  site_3       NO2  Ensemble   9.632655  6.706702  0.880852"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4e1c2d3-0644-4df6-b49d-e4fa7796e4df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site</th>\n",
              "      <th>Pollutant</th>\n",
              "      <th>Model</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>site_1</td>\n",
              "      <td>O3</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>6.436671</td>\n",
              "      <td>3.474949</td>\n",
              "      <td>0.935279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>site_1</td>\n",
              "      <td>NO2</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>9.712048</td>\n",
              "      <td>5.852114</td>\n",
              "      <td>0.857821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>site_2</td>\n",
              "      <td>O3</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>8.110899</td>\n",
              "      <td>5.317921</td>\n",
              "      <td>0.906114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>site_2</td>\n",
              "      <td>NO2</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>8.161874</td>\n",
              "      <td>5.637029</td>\n",
              "      <td>0.851812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>site_3</td>\n",
              "      <td>O3</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>11.565543</td>\n",
              "      <td>7.867597</td>\n",
              "      <td>0.901306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>site_3</td>\n",
              "      <td>NO2</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>9.632655</td>\n",
              "      <td>6.706702</td>\n",
              "      <td>0.880852</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4e1c2d3-0644-4df6-b49d-e4fa7796e4df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4e1c2d3-0644-4df6-b49d-e4fa7796e4df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4e1c2d3-0644-4df6-b49d-e4fa7796e4df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f5ff1978-9a66-4d15-8638-f678d2f478c0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f5ff1978-9a66-4d15-8638-f678d2f478c0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f5ff1978-9a66-4d15-8638-f678d2f478c0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f28c0f03-4628-4db1-8aff-4bee6468c315\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f28c0f03-4628-4db1-8aff-4bee6468c315 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "summary_df",
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Site\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"site_1\",\n          \"site_2\",\n          \"site_3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pollutant\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO2\",\n          \"O3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Ensemble\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.762315511193958,\n        \"min\": 6.436670838135665,\n        \"max\": 11.565542863596077,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          6.436670838135665\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4672476196440207,\n        \"min\": 3.474948888148252,\n        \"max\": 7.867597297751455,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3.474948888148252\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0316464210831416,\n        \"min\": 0.8518117872183826,\n        \"max\": 0.9352789504070186,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9352789504070186\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "final_results = []\n",
        "\n",
        "for file in site_files:\n",
        "    site_name = file.replace(\"_train_data.csv\", \"\")\n",
        "\n",
        "    X_test_seq = X_test_seqs[file]\n",
        "\n",
        "    for pollutant in [\"O3_target\", \"NO2_target\"]:\n",
        "        poll_short = pollutant.replace(\"_target\", \"\")\n",
        "\n",
        "        y_test = y_test_seqs[(file, pollutant)]\n",
        "        y_test_inv = scaler_ys[(file, pollutant)].inverse_transform(y_test)\n",
        "\n",
        "        model_dir = f\"saved_models/{site_name}\"\n",
        "\n",
        "        model_paths = {\n",
        "            \"LSTM\": os.path.join(model_dir, f\"{site_name}_{poll_short}_LSTM.keras\"),\n",
        "            \"GRU\": os.path.join(model_dir, f\"{site_name}_{poll_short}_GRU.keras\"),\n",
        "            \"Transformer\": os.path.join(model_dir, f\"{site_name}_{poll_short}_TRANS.keras\")\n",
        "        }\n",
        "\n",
        "        preds = {}\n",
        "\n",
        "        for model_name, path in model_paths.items():\n",
        "            model = load_model(path, compile=False)\n",
        "            y_pred_scaled = model.predict(X_test_seq)\n",
        "            y_pred = scaler_ys[(file, pollutant)].inverse_transform(y_pred_scaled)\n",
        "            preds[model_name] = y_pred.flatten()\n",
        "\n",
        "        # Compute ensemble\n",
        "        y_pred_ensemble = (\n",
        "            0.4 * preds[\"GRU\"] +\n",
        "            0.4 * preds[\"LSTM\"] +\n",
        "            0.2 * preds[\"Transformer\"]\n",
        "        )\n",
        "\n",
        "        preds[\"Ensemble\"] = y_pred_ensemble\n",
        "\n",
        "        # Store metrics\n",
        "        for model_name, pred in preds.items():\n",
        "            rmse = np.sqrt(mean_squared_error(y_test_inv, pred))   # FIXED\n",
        "            mae = mean_absolute_error(y_test_inv, pred)\n",
        "            r2 = r2_score(y_test_inv, pred)\n",
        "\n",
        "            final_results.append({\n",
        "                \"Site\": file,\n",
        "                \"Pollutant\": pollutant,\n",
        "                \"Model\": model_name,\n",
        "                \"RMSE\": rmse,\n",
        "                \"MAE\": mae,\n",
        "                \"R2\": r2\n",
        "            })\n",
        "\n",
        "\n",
        "final_df = pd.DataFrame(final_results)\n",
        "final_df.to_csv(\"FINAL_MODEL_METRICS.csv\", index=False)\n",
        "print(\"DONE  full metrics saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqaBLFTUVvpA",
        "outputId": "41a9ebd9-531e-4e28-d985-d9da2a29cae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m194/194\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
            "DONE  full metrics saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(final_results)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(results_df.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpclzvm6Wl66",
        "outputId": "1901d2f6-a157-46a1-9f80-a5d45f20f84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Site   Pollutant        Model       RMSE        MAE        R2\n",
            "0   site_1_train_data.csv   O3_target         LSTM   6.454074   3.224753  0.934928\n",
            "1   site_1_train_data.csv   O3_target          GRU   5.884445   3.092693  0.945908\n",
            "2   site_1_train_data.csv   O3_target  Transformer  12.180492   7.178348  0.768232\n",
            "3   site_1_train_data.csv   O3_target     Ensemble   6.436671   3.474949  0.935279\n",
            "4   site_1_train_data.csv  NO2_target         LSTM   9.345092   5.483871  0.868362\n",
            "5   site_1_train_data.csv  NO2_target          GRU   9.161602   5.263898  0.873481\n",
            "6   site_1_train_data.csv  NO2_target  Transformer  16.463231  11.229938  0.591451\n",
            "7   site_1_train_data.csv  NO2_target     Ensemble   9.712048   5.852114  0.857821\n",
            "8   site_2_train_data.csv   O3_target         LSTM   7.696978   5.054512  0.915452\n",
            "9   site_2_train_data.csv   O3_target          GRU   7.569438   4.739593  0.918231\n",
            "10  site_2_train_data.csv   O3_target  Transformer  16.691892  11.644270  0.602377\n",
            "11  site_2_train_data.csv   O3_target     Ensemble   8.110899   5.317921  0.906114\n",
            "12  site_2_train_data.csv  NO2_target         LSTM   8.152680   5.461892  0.852145\n",
            "13  site_2_train_data.csv  NO2_target          GRU   7.788624   5.220232  0.865055\n",
            "14  site_2_train_data.csv  NO2_target  Transformer  13.636210   9.928682  0.586361\n",
            "15  site_2_train_data.csv  NO2_target     Ensemble   8.161874   5.637029  0.851812\n",
            "16  site_3_train_data.csv   O3_target         LSTM  11.161983   7.167562  0.908074\n",
            "17  site_3_train_data.csv   O3_target          GRU  10.921623   6.956677  0.911990\n",
            "18  site_3_train_data.csv   O3_target  Transformer  24.098149  17.175866  0.571526\n",
            "19  site_3_train_data.csv   O3_target     Ensemble  11.565543   7.867597  0.901306\n",
            "20  site_3_train_data.csv  NO2_target         LSTM   9.614214   6.536862  0.881307\n",
            "21  site_3_train_data.csv  NO2_target          GRU   8.987694   5.872346  0.896273\n",
            "22  site_3_train_data.csv  NO2_target  Transformer  17.819242  13.418092  0.592268\n",
            "23  site_3_train_data.csv  NO2_target     Ensemble   9.632655   6.706702  0.880852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*GRAPH FOR RESEARCH PAPER*"
      ],
      "metadata": {
        "id": "oE9YEObOYhEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = results_df.copy()\n",
        "df[\"Site\"] = df[\"Site\"].str.replace(\"_train_data.csv\", \"\")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "colors = [\"#005f73\", \"#0a9396\"]   # RMSE, MAE\n",
        "color_r2 = \"#94d2bd\"\n",
        "\n",
        "# Helper: treat NaN as 0 but hide on plot\n",
        "def safe_value(val):\n",
        "    return 0 if pd.isna(val) else val\n",
        "\n",
        "\n",
        "def plot_rmse_mae(site_name):\n",
        "    site_df = df[df[\"Site\"] == site_name].copy()\n",
        "\n",
        "    site_df[\"RMSE\"] = site_df[\"RMSE\"].apply(safe_value)\n",
        "    site_df[\"MAE\"]  = site_df[\"MAE\"].apply(safe_value)\n",
        "\n",
        "    melt_df = site_df.melt(\n",
        "        id_vars=[\"Model\"],\n",
        "        value_vars=[\"RMSE\", \"MAE\"],\n",
        "        var_name=\"Metric\",\n",
        "        value_name=\"Value\"\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(\n",
        "        data=melt_df,\n",
        "        x=\"Model\",\n",
        "        y=\"Value\",\n",
        "        hue=\"Metric\",\n",
        "        palette=colors,\n",
        "        errorbar=None,\n",
        "        width=0.5     # REDUCED BAR WIDTH\n",
        "    )\n",
        "\n",
        "    # Add value labels\n",
        "    for p in ax.patches:\n",
        "        val = p.get_height()\n",
        "        if val != 0:\n",
        "            ax.annotate(\n",
        "                f\"{val:.2f}\",\n",
        "                (p.get_x() + p.get_width() / 2, val),\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=11,\n",
        "                fontweight=\"bold\"\n",
        "            )\n",
        "\n",
        "    plt.title(f\"{site_name}\", fontsize=18, fontweight=\"bold\")\n",
        "    plt.xlabel(\"Model\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.ylabel(\"Error Value\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "    plt.xticks(fontsize=12, fontweight=\"bold\")\n",
        "    plt.yticks(fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.legend(title=\"Metric\", title_fontsize=12, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_r2(site_name):\n",
        "    site_df = df[df[\"Site\"] == site_name].copy()\n",
        "    site_df[\"R2\"] = site_df[\"R2\"].apply(safe_value)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    ax = sns.barplot(\n",
        "        data=site_df,\n",
        "        x=\"Model\",\n",
        "        y=\"R2\",\n",
        "        color=color_r2,\n",
        "        errorbar=None,\n",
        "        width=0.5     # REDUCED BAR WIDTH\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        val = p.get_height()\n",
        "        if val != 0:\n",
        "            ax.annotate(\n",
        "                f\"{val:.2f}\",\n",
        "                (p.get_x() + p.get_width() / 2, val),\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=11,\n",
        "                fontweight=\"bold\"\n",
        "            )\n",
        "\n",
        "    plt.title(f\"{site_name}\", fontsize=18, fontweight=\"bold\")\n",
        "    plt.xlabel(\"Model\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.ylabel(\"R Score\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "    plt.xticks(fontsize=12, fontweight=\"bold\")\n",
        "    plt.yticks(fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Run for all 3 sites\n",
        "plot_rmse_mae(\"site_1\")\n",
        "plot_r2(\"site_1\")\n",
        "\n",
        "plot_rmse_mae(\"site_2\")\n",
        "plot_r2(\"site_2\")\n",
        "\n",
        "plot_rmse_mae(\"site_3\")\n",
        "plot_r2(\"site_3\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipn2wQK6XEp4",
        "outputId": "c4315e22-4731-432e-89ae-ffb3455abf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: Saved_Graphs/site_1_RMSE_MAE.png\n",
            "Saved: Saved_Graphs/site_1_R2.png\n",
            "Saved: Saved_Graphs/site_2_RMSE_MAE.png\n",
            "Saved: Saved_Graphs/site_2_R2.png\n",
            "Saved: Saved_Graphs/site_3_RMSE_MAE.png\n",
            "Saved: Saved_Graphs/site_3_R2.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"Saved_Graphs\", \"zip\", \"Saved_Graphs\")\n",
        "print(\"ZIP file created: Saved_Graphs.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoOaEGdlhwdr",
        "outputId": "15e5512f-9df5-4847-d230-e2d54db574ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file created: Saved_Graphs.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"Saved_Graphs.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "s5FMJhLTh3L6",
        "outputId": "76ea8722-7ec2-4d02-e636-8e47023188d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e74f6786-110e-4e84-b303-7ccf018cf7da\", \"Saved_Graphs.zip\", 382871)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your saved metrics file\n",
        "df = pd.read_csv(\"model_metrics_summary.csv\")\n",
        "\n",
        "# Clean up names\n",
        "df['Site'] = df['Site'].str.replace(\"_train_data.csv\", \"\")\n",
        "df['Pollutant'] = df['Pollutant'].str.replace(\"_target\", \"\")\n",
        "\n",
        "# Sort the table correctly\n",
        "df = df.sort_values(by=[\"Site\", \"Pollutant\", \"Model\"])\n",
        "\n",
        "# Function to print grouped table\n",
        "def print_grouped_table(df):\n",
        "    sites = df['Site'].unique()\n",
        "\n",
        "    for site in sites:\n",
        "        site_df = df[df['Site'] == site]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"{site.upper():^70}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"{'Site':<12} {'Pollutant':<12} {'Model':<15} {'RMSE':<10} {'MAE':<10} {'R2':<10}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        first_site_row = True\n",
        "        for pollutant in site_df['Pollutant'].unique():\n",
        "            pollutant_df = site_df[site_df['Pollutant'] == pollutant]\n",
        "\n",
        "            first_pollutant_row = True\n",
        "            for _, row in pollutant_df.iterrows():\n",
        "                print(\n",
        "                    f\"{site if first_site_row else '':<12} \"\n",
        "                    f\"{pollutant if first_pollutant_row else '':<12} \"\n",
        "                    f\"{row['Model']:<15} \"\n",
        "                    f\"{row['RMSE']:<10.3f} \"\n",
        "                    f\"{row['MAE']:<10.3f} \"\n",
        "                    f\"{row['R2']:<10.3f}\"\n",
        "                )\n",
        "                first_site_row = False\n",
        "                first_pollutant_row = False\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Print the formatted table\n",
        "print_grouped_table(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9hlxYNcmaJo",
        "outputId": "cfb7fbf2-ae52-4ae9-869b-070955f0984a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "                                SITE_1                                \n",
            "======================================================================\n",
            "Site         Pollutant    Model           RMSE       MAE        R2        \n",
            "----------------------------------------------------------------------\n",
            "site_1       NO2          Ensemble        9.712      5.852      0.858     \n",
            "             O3           Ensemble        6.437      3.475      0.935     \n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "                                SITE_2                                \n",
            "======================================================================\n",
            "Site         Pollutant    Model           RMSE       MAE        R2        \n",
            "----------------------------------------------------------------------\n",
            "site_2       NO2          Ensemble        8.162      5.637      0.852     \n",
            "             O3           Ensemble        8.111      5.318      0.906     \n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "                                SITE_3                                \n",
            "======================================================================\n",
            "Site         Pollutant    Model           RMSE       MAE        R2        \n",
            "----------------------------------------------------------------------\n",
            "site_3       NO2          Ensemble        9.633      6.707      0.881     \n",
            "                          GRU             8.988      5.872      0.896     \n",
            "                          LSTM            9.614      6.537      0.881     \n",
            "                          Transformer     17.819     13.418     0.592     \n",
            "             O3           Ensemble        11.566     7.868      0.901     \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"FINAL_MODEL_METRICS.csv\")\n",
        "\n",
        "df['Site'] = df['Site'].str.replace(\"_train_data.csv\", \"\")\n",
        "df['Pollutant'] = df['Pollutant'].str.replace(\"_target\",\"\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ptqpZIXUmv1_",
        "outputId": "7fc8c2d3-1126-4f77-ccda-19280e6fbc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Site Pollutant        Model       RMSE       MAE        R2\n",
              "0  site_1        O3         LSTM   6.454074  3.224753  0.934928\n",
              "1  site_1        O3          GRU   5.884445  3.092693  0.945908\n",
              "2  site_1        O3  Transformer  12.180492  7.178348  0.768232\n",
              "3  site_1        O3     Ensemble   6.436671  3.474949  0.935279\n",
              "4  site_1       NO2         LSTM   9.345092  5.483871  0.868362"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e210cb6d-eec1-47a9-bd28-995aeac6b3f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site</th>\n",
              "      <th>Pollutant</th>\n",
              "      <th>Model</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>site_1</td>\n",
              "      <td>O3</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>6.454074</td>\n",
              "      <td>3.224753</td>\n",
              "      <td>0.934928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>site_1</td>\n",
              "      <td>O3</td>\n",
              "      <td>GRU</td>\n",
              "      <td>5.884445</td>\n",
              "      <td>3.092693</td>\n",
              "      <td>0.945908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>site_1</td>\n",
              "      <td>O3</td>\n",
              "      <td>Transformer</td>\n",
              "      <td>12.180492</td>\n",
              "      <td>7.178348</td>\n",
              "      <td>0.768232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>site_1</td>\n",
              "      <td>O3</td>\n",
              "      <td>Ensemble</td>\n",
              "      <td>6.436671</td>\n",
              "      <td>3.474949</td>\n",
              "      <td>0.935279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>site_1</td>\n",
              "      <td>NO2</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>9.345092</td>\n",
              "      <td>5.483871</td>\n",
              "      <td>0.868362</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e210cb6d-eec1-47a9-bd28-995aeac6b3f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e210cb6d-eec1-47a9-bd28-995aeac6b3f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e210cb6d-eec1-47a9-bd28-995aeac6b3f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-21d1e02c-e4e3-4875-aa58-d07533b72e0d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21d1e02c-e4e3-4875-aa58-d07533b72e0d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-21d1e02c-e4e3-4875-aa58-d07533b72e0d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"Site\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"site_1\",\n          \"site_2\",\n          \"site_3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pollutant\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO2\",\n          \"O3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"GRU\",\n          \"Ensemble\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.3131712389558325,\n        \"min\": 5.884445071485211,\n        \"max\": 24.09814896955739,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          7.696977964133709,\n          11.161983222131449\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3800029965782055,\n        \"min\": 3.092692906129439,\n        \"max\": 17.175866315899714,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          5.054512190352105,\n          7.16756197314511\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12938406098032307,\n        \"min\": 0.5715264214361306,\n        \"max\": 0.94590788319077,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.9154524496665932,\n          0.9080737788115862\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_grouped_table(df):\n",
        "    sites = df['Site'].unique()\n",
        "\n",
        "    for site in sites:\n",
        "        site_df = df[df['Site'] == site]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"{site.upper():^70}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"{'Site':<12} {'Pollutant':<12} {'Model':<15} {'RMSE':<10} {'MAE':<10} {'R2':<10}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        first_site_row = True\n",
        "        for pollutant in site_df['Pollutant'].unique():\n",
        "            pollutant_df = site_df[site_df['Pollutant'] == pollutant]\n",
        "\n",
        "            first_pollutant_row = True\n",
        "            for _, row in pollutant_df.iterrows():\n",
        "                print(\n",
        "                    f\"{site if first_site_row else '':<12} \"\n",
        "                    f\"{pollutant if first_pollutant_row else '':<12} \"\n",
        "                    f\"{row['Model']:<15} \"\n",
        "                    f\"{row['RMSE']:<10.3f} \"\n",
        "                    f\"{row['MAE']:<10.3f} \"\n",
        "                    f\"{row['R2']:<10.3f}\"\n",
        "                )\n",
        "                first_site_row = False\n",
        "                first_pollutant_row = False\n",
        "        print(\"\\n\")\n",
        "\n",
        "print_grouped_table(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR4EwcHVmrVP",
        "outputId": "6ebdb9a3-7471-468a-ad7d-44d13ffdab90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "                                SITE_1                                \n",
            "======================================================================\n",
            "Site         Pollutant    Model           RMSE       MAE        R2        \n",
            "----------------------------------------------------------------------\n",
            "site_1       O3           LSTM            6.454      3.225      0.935     \n",
            "                          GRU             5.884      3.093      0.946     \n",
            "                          Transformer     12.180     7.178      0.768     \n",
            "                          Ensemble        6.437      3.475      0.935     \n",
            "             NO2          LSTM            9.345      5.484      0.868     \n",
            "                          GRU             9.162      5.264      0.873     \n",
            "                          Transformer     16.463     11.230     0.591     \n",
            "                          Ensemble        9.712      5.852      0.858     \n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "                                SITE_2                                \n",
            "======================================================================\n",
            "Site         Pollutant    Model           RMSE       MAE        R2        \n",
            "----------------------------------------------------------------------\n",
            "site_2       O3           LSTM            7.697      5.055      0.915     \n",
            "                          GRU             7.569      4.740      0.918     \n",
            "                          Transformer     16.692     11.644     0.602     \n",
            "                          Ensemble        8.111      5.318      0.906     \n",
            "             NO2          LSTM            8.153      5.462      0.852     \n",
            "                          GRU             7.789      5.220      0.865     \n",
            "                          Transformer     13.636     9.929      0.586     \n",
            "                          Ensemble        8.162      5.637      0.852     \n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "                                SITE_3                                \n",
            "======================================================================\n",
            "Site         Pollutant    Model           RMSE       MAE        R2        \n",
            "----------------------------------------------------------------------\n",
            "site_3       O3           LSTM            11.162     7.168      0.908     \n",
            "                          GRU             10.922     6.957      0.912     \n",
            "                          Transformer     24.098     17.176     0.572     \n",
            "                          Ensemble        11.566     7.868      0.901     \n",
            "             NO2          LSTM            9.614      6.537      0.881     \n",
            "                          GRU             8.988      5.872      0.896     \n",
            "                          Transformer     17.819     13.418     0.592     \n",
            "                          Ensemble        9.633      6.707      0.881     \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5jJkBPZnVYX",
        "outputId": "1ea92980-11ff-4af4-d0fb-153460aa9f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"FINAL_MODEL_METRICS.csv\")\n",
        "\n",
        "df['Site'] = df['Site'].str.replace(\"_train_data.csv\", \"\")\n",
        "df['Pollutant'] = df['Pollutant'].str.replace(\"_target\",\"\")\n",
        "\n",
        "document = Document()\n",
        "\n",
        "sites = df['Site'].unique()\n",
        "\n",
        "for site in sites:\n",
        "    document.add_heading(f\"Performance Metrics  {site.upper()}\", level=1)\n",
        "\n",
        "    site_df = df[df['Site'] == site]\n",
        "\n",
        "    table = document.add_table(rows=1, cols=6)\n",
        "    hdr = table.rows[0].cells\n",
        "    hdr[0].text = \"Site\"\n",
        "    hdr[1].text = \"Pollutant\"\n",
        "    hdr[2].text = \"Model\"\n",
        "    hdr[3].text = \"RMSE\"\n",
        "    hdr[4].text = \"MAE\"\n",
        "    hdr[5].text = \"R\"\n",
        "\n",
        "    for _, row in site_df.iterrows():\n",
        "        row_cells = table.add_row().cells\n",
        "        row_cells[0].text = row[\"Site\"]\n",
        "        row_cells[1].text = row[\"Pollutant\"]\n",
        "        row_cells[2].text = row[\"Model\"]\n",
        "        row_cells[3].text = f\"{row['RMSE']:.3f}\"\n",
        "        row_cells[4].text = f\"{row['MAE']:.3f}\"\n",
        "        row_cells[5].text = f\"{row['R2']:.3f}\"\n",
        "\n",
        "    document.add_page_break()\n",
        "\n",
        "document.save(\"MODEL_METRICS_TABLE.docx\")\n",
        "print(\"Saved MODEL_METRICS_TABLE.docx\")\n"
      ],
      "metadata": {
        "id": "W3hkAmTlnZfr",
        "outputId": "745c8ff5-9e6e-43d7-d508-b2b18d812df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved MODEL_METRICS_TABLE.docx\n"
          ]
        }
      ]
    }
  ]
}